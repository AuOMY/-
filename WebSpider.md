# 网络爬虫--房价分析

## 数据爬取
项目的数据来源于贝壳网佛山市顺德区 (https://fs.ke.com/ershoufang/shunde/) 的房价信息，主要采用了python中的requests库来进行网页租房信息的爬取。  
这里主要涉及的是关于浏览器网页源代码的分析并书写XPath路径这一难点，或者说是关键点，网页源代码应该是用到Javascript来书写的(应该是吧)，网页Xpath路径主要分为两条，一条是绝对路径，从html源地址出发到相关信息存储地址，另一条则是相对路径，相对于某一页面出发寻找相关信息。  

### python库
—requests库主要用来爬取网页租房信息  
—time库则是对爬取时间进行一定的设置，因为网站会对爬取进行一定的限制，设置时间会提高效率以及防止被封IP(实验中并没有用到time库也是可以进行爬取的，但是可能存在多次访问才能爬取成功的情况，虽然并没有显示，但网站确实有对频繁访问进行一定的时间约束，所以更加建议设置间隔时间，如0.5s访问一次)  
—Pool库是实现多线程的，多线程爬取也能够提高爬取的效率  
—etree库是同Beautiful Soup库类似的库，可以对爬取网站的源代码进行解析  
—pandas库和os库都是用来存储爬取的信息，由于pandas存储的一定特点，加入os的辅助使得存储更加优化  
—random库是用来实现随机访问IP的，防止IP被封(实验中也并没有用到，因为多个IP实现起来比较麻烦，单IP在少量少次爬取信息时风险还是比较低的)  

### 伪装
设置UserAgent，伪装成浏览器对网页进行访问，爬取的网站是贝壳网顺德区的二手房网站，对爬虫进行伪装，避免服务器对程序的拒绝访问，也就不会发送相关的信息，但UserAgent的设置可以不唯一(通过设置随机数进行选取)，此处仅设置一个UserAgent  

### 爬取范围
经过几次爬虫的尝试，在房价信息爬取到100页之后，数据就会出现重复，所以爬取页码直接设置从0到100页，相当于对数据进行了一定的去重处理  

### 属性选择
可以自行选择所需房价属性进行爬取，但秉着“宁可错杀一百，不能放过一千”的原则，选择将一房的所有信息都爬取下来，会更加方便后续的数据处理，当然后面数据处理部分会进行属性的筛选。其中也包含了XPath路径的书写，如果已经获取到每一页的网站，就能够利用的相对路径进行寻址，也可以选择绝对寻址，个人选择  

### 编码格式
此处利用的是‘UTF-8-SIG’的编码方式，利用‘UTF-8’的编码方式存储会导致除了英文其余数据显示乱码的现象，当然，也可以根据网站编码方式进行编码  

### 数据存取
利用列表增长+pandas库存储文件对爬虫数据进行存储 

## 数据处理
### 数据清洗
此处爬虫数据几乎完整“干净”，仅存在极个别空白、乱码或者不全数据，因为并不会对分析造成影响，可以直接手动清除，由于房屋数据并不是单纯的数字信息，所以能够直接删除，但总体上是不建议删除数据，应该合理填充，特别是纯数字数据

### 数据读取
对于已经生成的csv数据文件，如果直接使用pandas库进行read操作，有可能会出现编码错误   
无论是使用UTF-8编码还是其他编码方式进行read操作都无法成功读取，此时在csv文件的开头加入 # -*- coding: utf-8 -*- 能够对文件编码方式进行改变，再利用UTF-8的编码方式进行读取则可成功读取  
但是如果放在表头会使得列名被更改，在数据读取时会造成影响，所以也可以将其放在表尾，在数据处理时手动忽略表尾的 # -*- coding: utf-8 -*-   
